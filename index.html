<!doctype html>
<html lang="en" class="h-100">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="Dimitris Menexopoulos">
    <title>The State of the Art in Procedural Audio</title>
    
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
          integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
    <meta name="theme-color" content="#563d7c">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.0/font/bootstrap-icons.css">
</head>
    
<body>  
    <!-- Page Content -->
<div class="text-center">
    <div class="row">
        <div class="col-lg-12">
            <br>
            <h1 class="mt-5 mb-3">The State of the Art in Procedural Audio</h1>
            <p class="lead">
                <a href="https://linktr.ee/menexmusic" target=”_blank”>Dimitris Menexopoulos<sup>1</sup></a>,
                <a href="https://www.cienciavitae.pt/portal/2714-8A7B-5CCA" target=”_blank”>Pedro D. Pestana<sup>2</sup></a>, and
                <a href="https://www.eecs.qmul.ac.uk/~josh/" target=”_blank”>Joshua D. Reiss<sup>1</sup></a>
            </p>
            <p class="lead">
                <sup>1</sup> Centre for Digital Music, Queen Mary University of London<br>
                <sup>2</sup> Science and Technology Department, The Open University of Portugal (UAb) <br>
            </p> 
            <div>
                <a class="btn btn-outline-dark" href="https://dmenex.github.io/proceduralaudioreview/" target=”_blank” type="button">
                    <i class="bi bi-newspaper"></i> Paper</a>
                <a class="btn btn-outline-dark" href="https://github.com/dmenex/proceduralaudioreview" target=”_blank”
                   type="button">
                    <i class="bi bi-code-slash"></i> Code</a>
            </div>
        </div>
    </div>
</div>

<div align="center">
    <br>
    <br>
    <iframe width="840" height="472" src="https://www.youtube.com/embed/E4jEOTke8FY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
 </div>
    
<div class="col-lg-6" style="float:none;margin:auto;">    
    <div class="section mt-5 mb-5">
         <div class="abstract">
              <h2>Abstract</h2>
                   <hr>
                   <p align="justify">
                         Procedural audio may be defined as real-time sound generation according to programmatic rules and live input. 
                         It is often considered a subset of sound synthesis and is especially applicable to nonlinear media, such as video games, virtual reality experiences and interactive audiovisual installations. 
                         However, there is resistance to widespread adoption of procedural audio because there is little awareness of the state of the art, including the diversity of sounds that may be generated, 
                         the controllability of procedural audio models and the quality of the sounds that it produces. 
                         We address all of these aspects in this review paper, while attempting a large scale categorisation of sounds that have been approached through procedural audio techniques. 
                         The role of recent advancements in neural audio synthesis, its current implementations, as well as potential future applications in the field are also discussed.
                   </p>
         </div>
    </div>
</div> 
                                     
<div class="col-lg-6" style="float:none;margin:auto;">    
    
         <div class="Figures">
              <h2>Figures</h2>
                <hr>
             <div class="text-center" style="float:none;margin:auto;">
                 
                 <img class="figure1" src="Figures\Fig. 1.png" style="width:100%;">
                 <p>
                    Fig. 1: Flowchart highlighting the study selection process
                    and the number of articles selected at each step.          
                 </p>
                 
               <img class="figure2" src="Figures\Fig. 2.png" style="width:100%;">
                <p>
                    Fig. 2: Number of articles published since 1993 including 
                    a practical algorithm in procedural audio, broken down into our main taxonomies.   
                </p>
                 
               <img class="figure3" src="Figures\Fig. 3.png" style="width:100%;">
                 <p>
                    Fig. 3: An analysis of how the number of citations for
                    each paper is spread across our main taxonomy,
                    highlighting what seems to be considered more relevant.
                    Closer to the left one can see the distribution of taxonomic
                    themes along articles that have been cited fewer times. To
                    the right, articles that have been cited over 100 times are
                    mainly distributed between two taxons (Machine and
                    Contact Sounds).          
                 </p>
                 
                <img class="figure4" src="Figures\Fig. 4.png" style="width:100%;">
                 <p>
                    Fig. 4: Relationship between the design strategies used to build the model (y-axis) and the synthesis type used (x-axis)
                    for the articles in Table 1. Several articles will make use of more than one methodology and synthesis type, therefore this
                    heatmap does not have a one-to-one relationship with the articles (i.e. there are many more items here than the total
                    number of articles analyzed).   
                 </p>
                 
               <img class="figure5" src="Figures\Fig. 5.png" style="width:100%;">
                 <p>
                    Fig. 5: Relationship between the design strategies used to build the model (x-axis) and the sound type characterization
                    (y-axis) for the articles in Table 1.   
                 </p>   
            </div>
         </div>

</div> 
                                                             
 <div class="col-lg-6" style="float:none;margin:auto;">    
    
         <div class="Tables">
              <h2>Tables</h2>
                <hr>
             <div class="text-center" style="float:none;margin:auto;">
                 <img class="table1" src="Tables\Table 1.png" style="width:100%;">
                 <p>
                    Table 1: A summary of the state of the art in the field of procedural audio. Numbers in each cell correspond to reviewed
                    references in which a sound class is connected with a specific synthesis technique. n=x unique occurrences in the
                    bibliography are provided for both the broad and narrow taxonomies, as well as for the synthesis techniques.
                </p> 
                 <img class="table2" src="Tables\Table 2.png" style="width:100%;">
                 <p>
                    Table 2: State of the art in procedural audio evaluation. The
                    references from 1 are classified by evaluation type. The last
                    row gives papers from either subjective evaluation category
                    (comparative or non-comparative) that also contain objective
                    evaluation. The last column gives the average publication
                    year of all papers in a category.
                </p>                                              
            </div>
         </div>
 
</div>
                                                            
<div class="col-lg-6" style="float:none;margin:auto;">   
      <div class="Citation">
              <h2>Citation</h2>
              <hr>
        <p align="justify">
             Accepted at the <a href="https://www.aes.org/journal/" target=”_blank”>Journal of the Audio Engineering Society (JAES).</a> 
        </p>
        <p align="justify">
             Supported by the <a href="https://www.iggi-phd.org" target=”_blank”>EPSRC Centre for Doctoral Training in Intelligent Games and Game Intelligence (iGGi) [EP/S022325/1].</a>
        </p>
        <br>
       </div>
</div>
    
    <!-- Bootstrap core JavaScript -->
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>
<script src="js/slider.js"></script>
</body>
    
</html>
